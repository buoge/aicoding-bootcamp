from typing import List
from sqlalchemy import create_engine, text
from sqlalchemy.orm import Session
from openai import OpenAI

from app.db import metadata_store
from app.models.nl_query import NLQueryRequest, NLQueryResponse
from app.models.connection import TableInfo
from app.services.sql_guard import validate_and_patch
from app.services.nl2sql_prompt import build_context
from app.core.config import get_settings
import json


def _pick_table(tables: List[TableInfo]) -> TableInfo:
    if not tables:
        raise ValueError("No metadata available for this connection")
    # simple heuristic: first table
    return tables[0]


def generate_sql(prompt: str, tables: List[TableInfo], api_key: str | None) -> str:
    settings = get_settings()
    key = api_key or settings.deepseek_api_key
    if not key:
        raise ValueError("DeepSeek API key is missing")

    client = OpenAI(api_key=key, base_url=settings.deepseek_base_url)
    table = _pick_table(tables)
    context = build_context(tables)
    messages = [
        {
            "role": "system",
            "content": (
                "You are an assistant that generates ONLY safe SQL SELECT statements for Postgres.\n"
                "- Use provided metadata; schema.table names must match exactly.\n"
                "- Do not generate INSERT/UPDATE/DELETE/DDL.\n"
                "- Always include an explicit LIMIT (<=1000) if user didn't specify.\n"
                "- Return SQL only, no explanation."
            ),
        },
        {
            "role": "user",
            "content": f"Metadata:\n{context}\nUser request: {prompt}\nGenerate a SELECT for Postgres.",
        },
    ]
    response = client.chat.completions.create(
        model=settings.deepseek_model,
        messages=messages,
        stream=False,
    )
    sql = response.choices[0].message.content.strip()
    # Fallback if empty
    if not sql:
        sql = f"SELECT * FROM {table.schema}.{table.name} LIMIT 100"
    return sql


def generate_and_run(db: Session, payload: NLQueryRequest) -> NLQueryResponse:
    metadata_store.init_db()
    meta = metadata_store.get_metadata(db, payload.connection_id)
    if not meta:
        raise ValueError("Connection not found")
    conn, raw_tables = meta

    # parse stored tables to TableInfo
    tables: List[TableInfo] = []
    for t in raw_tables:
        tables.append(
            TableInfo(
                schema=t.schema,
                name=t.name,
                is_view=t.is_view,
                columns=[c for c in (t.columns_json and json.loads(t.columns_json) or [])],
            )
        )

    context = build_context(tables)
    generated_sql = generate_sql(payload.prompt, tables, payload.api_key)

    patched_sql, limit_added = validate_and_patch(generated_sql)

    engine = create_engine(conn.connection_url)
    with engine.connect() as connection:
        result = connection.execute(text(patched_sql))
        rows = result.fetchall()
        columns = [col for col in result.keys()]

    return NLQueryResponse(
        generatedSql=generated_sql,
        columns=columns,
        rows=[list(r) for r in rows],
        limitAdded=limit_added,
        message=f"Generated by DeepSeek using provided metadata context.",
    )

